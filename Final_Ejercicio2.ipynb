{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# utilizado para la manipulación de directorios y rutas\n",
    "import os\n",
    "\n",
    "# Cálculo científico y vectorial para python\n",
    "import numpy as np\n",
    "\n",
    "# Libreria para graficos\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Modulo de optimizacion en scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# le dice a matplotlib que incruste gráficos en el cuaderno\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers):\n",
    "        # el MLP es una lista de capas\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculamos la salida del modelo aplicando\n",
    "        # cada capa de manera secuencial\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # por defecto, devolver los inputs\n",
    "        # cada capa hará algo diferente aquí\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # cada capa, calculará sus gradientes\n",
    "        # y los devolverá para las capas siguientes\n",
    "        return grad\n",
    "\n",
    "    def update(self, params):\n",
    "        # si hay parámetros, los actualizaremos\n",
    "        # con lo que nos de el optimizer\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        # pesos de la capa\n",
    "        self.w = np.random.normal(loc=0.0,\n",
    "        scale=np.sqrt(2/(d_in+d_out)),\n",
    "        size=(d_in, d_out))\n",
    "        self.b = np.zeros(d_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        self.params = [self.w, self.b]\n",
    "        # salida del preceptrón\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # gradientes para la capa siguiente (BACKPROP)\n",
    "        grad = np.dot(grad_output, self.w.T)\n",
    "        self.grad_w = np.dot(self.x.T, grad_output)\n",
    "        # gradientes para actualizar pesos\n",
    "        self.grad_b = grad_output.mean(axis=0)*self.x.shape[0]\n",
    "        self.grads = [self.grad_w, self.grad_b]\n",
    "        return grad\n",
    "\n",
    "    def update(self, params):\n",
    "        self.w, self.b = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciones de activacion\n",
    "class ReLU(Layer):\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad = self.x > 0\n",
    "        return grad_output*grad\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return sigmoid(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad = sigmoid(self.x)*(1 - sigmoid(self.x))\n",
    "        return grad_output*grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizador, stocastic greadent descent\n",
    "class SGD():\n",
    "    def __init__(self, net, lr):\n",
    "        self.net = net\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self):\n",
    "        for layer in self.net.layers:\n",
    "            layer.update([\n",
    "                params - self.lr*grads\n",
    "                for params, grads in zip(layer.params, layer.grads)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss():\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "\n",
    "    def backward(self):\n",
    "        # derivada de la loss function con respecto\n",
    "        # a la salida del MLP\n",
    "        grad = self.grad_loss()\n",
    "        # BACKPROPAGATION\n",
    "        for layer in reversed(self.net.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "class MSE(Loss):\n",
    "    def __call__(self, output, target):\n",
    "        self.output, self.target = output, target.reshape(output.shape)\n",
    "        loss = np.mean((self.output - self.target)**2)\n",
    "        return loss.mean()\n",
    "\n",
    "    def grad_loss(self):\n",
    "        return self.output -  self.target\n",
    "\n",
    "class BCE(Loss):\n",
    "    def __call__(self, output, target):\n",
    "        self.output, self.target = output, target.reshape(output.shape)\n",
    "        loss = - np.mean(self.target*np.log(self.output) - (1 - self.target)*np.log(1 - self.output))\n",
    "        return loss.mean()\n",
    "\n",
    "    def grad_loss(self):\n",
    "        return self.output -  self.target\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    def __call__(self, output, target):\n",
    "        self.output, self.target = output, target\n",
    "        logits = output[np.arange(len(output)), target]\n",
    "        loss = - logits + np.log(np.sum(np.exp(output), axis=-1))\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    def grad_loss(self):\n",
    "        answers = np.zeros_like(self.output)\n",
    "        answers[np.arange(len(self.output)), self.target] = 1\n",
    "        return (- answers + softmax(self.output)) / self.output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "#Carga de dataset\n",
    "data = pd.read_csv('Brain Tumor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3762 entries, 0 to 3761\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Image               3762 non-null   object \n",
      " 1   Class               3762 non-null   int64  \n",
      " 2   Mean                3762 non-null   float64\n",
      " 3   Variance            3762 non-null   float64\n",
      " 4   Standard Deviation  3762 non-null   float64\n",
      " 5   Entropy             3762 non-null   float64\n",
      " 6   Skewness            3762 non-null   float64\n",
      " 7   Kurtosis            3762 non-null   float64\n",
      " 8   Contrast            3762 non-null   float64\n",
      " 9   Energy              3762 non-null   float64\n",
      " 10  ASM                 3762 non-null   float64\n",
      " 11  Homogeneity         3762 non-null   float64\n",
      " 12  Dissimilarity       3762 non-null   float64\n",
      " 13  Correlation         3762 non-null   float64\n",
      " 14  Coarseness          3762 non-null   float64\n",
      "dtypes: float64(13), int64(1), object(1)\n",
      "memory usage: 441.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_eiliminar = ['Image']\n",
    "data = data.drop(columnas_eiliminar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3762 entries, 0 to 3761\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Class               3762 non-null   int64  \n",
      " 1   Mean                3762 non-null   float64\n",
      " 2   Variance            3762 non-null   float64\n",
      " 3   Standard Deviation  3762 non-null   float64\n",
      " 4   Entropy             3762 non-null   float64\n",
      " 5   Skewness            3762 non-null   float64\n",
      " 6   Kurtosis            3762 non-null   float64\n",
      " 7   Contrast            3762 non-null   float64\n",
      " 8   Energy              3762 non-null   float64\n",
      " 9   ASM                 3762 non-null   float64\n",
      " 10  Homogeneity         3762 non-null   float64\n",
      " 11  Dissimilarity       3762 non-null   float64\n",
      " 12  Correlation         3762 non-null   float64\n",
      " 13  Coarseness          3762 non-null   float64\n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 411.6 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3762, 14)\n",
      "(3762,)\n",
      "      Class       Mean     Variance  Standard Deviation   Entropy  Skewness  \\\n",
      "0         0   6.535339   619.587845           24.891522  0.109059  4.276477   \n",
      "1         0   8.749969   805.957634           28.389393  0.266538  3.718116   \n",
      "2         1   7.341095  1143.808219           33.820234  0.001467  5.061750   \n",
      "3         1   5.958145   959.711985           30.979219  0.001477  5.677977   \n",
      "4         0   7.315231   729.540579           27.010009  0.146761  4.283221   \n",
      "...     ...        ...          ...                 ...       ...       ...   \n",
      "3757      0  21.234512  1208.850174           34.768523  0.063774  2.082079   \n",
      "3758      0  20.435349  1227.151440           35.030721  0.066763  2.144625   \n",
      "3759      0  18.011520  1151.582765           33.934978  0.068396  2.308349   \n",
      "3760      0  13.330429   945.732779           30.752769  0.087872  2.732822   \n",
      "3761      0   6.110138   480.884025           21.929068  0.118171  4.110669   \n",
      "\n",
      "       Kurtosis    Contrast    Energy       ASM  Homogeneity  Dissimilarity  \\\n",
      "0     18.900575   98.613971  0.293314  0.086033     0.530941       4.473346   \n",
      "1     14.464618   63.858816  0.475051  0.225674     0.651352       3.220072   \n",
      "2     26.479563   81.867206  0.031917  0.001019     0.268275       5.981800   \n",
      "3     33.428845  151.229741  0.032024  0.001026     0.243851       7.700919   \n",
      "4     19.079108  174.988756  0.343849  0.118232     0.501140       6.834689   \n",
      "...         ...         ...       ...       ...          ...            ...   \n",
      "3757   4.647310  158.437600  0.220666  0.048693     0.487131       5.211739   \n",
      "3758   4.882034  161.158675  0.225931  0.051045     0.502712       5.083126   \n",
      "3759   5.579498  167.130118  0.228930  0.052409     0.492269       5.103700   \n",
      "3760   7.757570  223.812932  0.261527  0.068397     0.480064       6.439784   \n",
      "3761  17.538826  239.251388  0.306224  0.093773     0.494333       6.787329   \n",
      "\n",
      "      Correlation     Coarseness  \n",
      "0        0.981939  7.458341e-155  \n",
      "1        0.988834  7.458341e-155  \n",
      "2        0.978014  7.458341e-155  \n",
      "3        0.964189  7.458341e-155  \n",
      "4        0.972789  7.458341e-155  \n",
      "...           ...            ...  \n",
      "3757     0.950972  7.458341e-155  \n",
      "3758     0.952749  7.458341e-155  \n",
      "3759     0.952181  7.458341e-155  \n",
      "3760     0.940898  7.458341e-155  \n",
      "3761     0.938731  7.458341e-155  \n",
      "\n",
      "[3762 rows x 14 columns]\n",
      "0       0\n",
      "1       0\n",
      "2       1\n",
      "3       1\n",
      "4       0\n",
      "       ..\n",
      "3757    0\n",
      "3758    0\n",
      "3759    0\n",
      "3760    0\n",
      "3761    0\n",
      "Name: Class, Length: 3762, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = data.iloc[:, 0:]\n",
    "y = data.iloc[:, 0]\n",
    "m = y.size\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un nuevo DataFrame con los datos modificados\n",
    "nuevo_data = data.copy()\n",
    "# Guardar el dataset actualizado en un nuevo archivo\n",
    "nuevo_data.to_csv('tumor.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3762, 13)\n",
      "(3762,)\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"tumor.csv\", delimiter=',',skiprows=1)\n",
    "# print(data)\n",
    "x, y = data[:, :13].astype(int), data[:, 0].astype(int)\n",
    "x = x.reshape(len(x),13)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlope\\AppData\\Local\\Temp\\ipykernel_11668\\2052036624.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  X_norm = (x - X_mean) / X_std\n"
     ]
    }
   ],
   "source": [
    "# #Normalización entre 0 y 1\n",
    "\n",
    "X_mean, X_std = x.mean(axis=0), x.std(axis=0)\n",
    "X_norm = (x - X_mean) / X_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1881,), (1883,))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = x[:1881] , x[1879:] , y[:1881].astype(np.int64), y[1879:].astype(np.int64)\n",
    "x_train.shape , x_test.shape\n",
    "\n",
    "y_train.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de activacion para la capa oculta del mlp\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def reluPrime(x):\n",
    "  return x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy -> usada para clasificación binaria (con sigmoid)\n",
    "def bce(y, y_hat):\n",
    "    return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_bce(y, y_hat):\n",
    "    return y_hat - y.reshape(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clase base MLP\n",
    "\n",
    "class MLP():\n",
    "  def __init__(self, D_in, H, D_out, loss, grad_loss, activation):\n",
    "    # pesos de la capa 1\n",
    "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
    "    scale=np.sqrt(2/(D_in+H)),\n",
    "    size=(D_in, H)), np.zeros(H)\n",
    "    # pesos de la capa 2\n",
    "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
    "    scale=np.sqrt(2/(H+D_out)),\n",
    "    size=(H, D_out)), np.zeros(D_out)\n",
    "    self.ws = []\n",
    "    # función de pérdida y derivada\n",
    "    self.loss = loss\n",
    "    self.grad_loss = grad_loss\n",
    "    # función de activación\n",
    "    self.activation = activation\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # salida de la capa 1\n",
    "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
    "    self.h = relu(self.h_pre)\n",
    "    # salida del MLP\n",
    "    y_hat = np.dot(self.h, self.w2) + self.b2\n",
    "    return self.activation(y_hat)\n",
    "\n",
    "  def fit(self, X, Y, epochs = 100, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
    "    batch_size = len(X) if batch_size == None else batch_size\n",
    "    batches = len(X) // batch_size\n",
    "    l = []\n",
    "    for e in range(1,epochs+1):\n",
    "        # Mini-Batch Gradient Descent\n",
    "        _l = []\n",
    "        for b in range(batches):\n",
    "            # batch de datos\n",
    "            x = X[b*batch_size:(b+1)*batch_size]\n",
    "            y = Y[b*batch_size:(b+1)*batch_size]\n",
    "            # salida del perceptrón\n",
    "            y_pred = self(x)\n",
    "            # función de pérdida\n",
    "            loss = self.loss(y, y_pred)\n",
    "            _l.append(loss)\n",
    "            # Backprop\n",
    "            dldy = self.grad_loss(y, y_pred)\n",
    "            grad_w2 = np.dot(self.h.T, dldy)\n",
    "            grad_b2 = dldy.mean(axis=0)\n",
    "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)\n",
    "            grad_w1 = np.dot(x.T, dldh)\n",
    "            grad_b1 = dldh.mean(axis=0)\n",
    "            # Update (GD)\n",
    "            self.w1 = self.w1 - lr * grad_w1\n",
    "            self.b1 = self.b1 - lr * grad_b1\n",
    "            self.w2 = self.w2 - lr * grad_w2\n",
    "            self.b2 = self.b2 - lr * grad_b2\n",
    "        l.append(np.mean(_l))\n",
    "        # guardamos pesos intermedios para visualización\n",
    "        self.ws.append((\n",
    "            self.w1.copy(),\n",
    "            self.b1.copy(),\n",
    "            self.w2.copy(),\n",
    "            self.b2.copy()\n",
    "        ))\n",
    "        if verbose and not e % log_each:\n",
    "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
    "\n",
    "  def predict(self, ws, x):\n",
    "    w1, b1, w2, b2 = ws\n",
    "    h = relu(np.dot(x, w1) + b1)\n",
    "    y_hat = np.dot(h, w2) + b2\n",
    "    return self.activation(y_hat)\n",
    "  def evaluate(perceptron, x, t = 0.5):\n",
    "    w = perceptron.ws[-1]\n",
    "    x = np.c_[np.ones(len(x)), x]\n",
    "    y = perceptron(w, x)\n",
    "    return (y > t).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP para clasificación binaria\n",
    "class MLPBinaryClassification(MLP):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__(D_in, H, D_out, bce, grad_bce, sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlope\\AppData\\Local\\Temp\\ipykernel_11668\\3964504945.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))\n",
      "C:\\Users\\hlope\\AppData\\Local\\Temp\\ipykernel_11668\\3964504945.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100, Loss: nan\n",
      "Epoch: 20/100, Loss: nan\n",
      "Epoch: 30/100, Loss: nan\n",
      "Epoch: 40/100, Loss: nan\n",
      "Epoch: 50/100, Loss: nan\n",
      "Epoch: 60/100, Loss: nan\n",
      "Epoch: 70/100, Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80/100, Loss: nan\n",
      "Epoch: 90/100, Loss: nan\n",
      "Epoch: 100/100, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = MLPBinaryClassification(D_in=13, H=3, D_out=1)\n",
    "epochs, lr = 50, 0.1\n",
    "\n",
    "# normalización datos\n",
    "x_mean = np.mean(x_train)\n",
    "x_std = np.std(x_train)\n",
    "x_std = np.nan_to_num(x_std, nan=1.0)\n",
    "\n",
    "\n",
    "# Calcula x_norm\n",
    "x_norm = (x_train - x_mean) / x_std\n",
    "\n",
    "model.fit(x, y, epochs=100, batch_size=50, log_each=10)\n",
    "#model.fit(x_norm, y_train, epochs, lr, batch_size=1, log_each=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.19074007,   0.90424297,   0.16686989],\n",
       "        [  0.57303163,  -0.60169527,  -0.0532902 ],\n",
       "        [ -1.55593072, -10.76948315,  -0.7516239 ],\n",
       "        [  0.09528325,  -0.18350034,  -0.40441721],\n",
       "        [ -0.17989147,  -0.06710653,  -0.14040583],\n",
       "        [ -0.06356044,   0.26754762,  -0.25567709],\n",
       "        [ -0.22925651,  -0.3852454 ,  -1.01022306],\n",
       "        [ -0.56666488,  -0.87362061,  -2.09871584],\n",
       "        [ -0.67192007,   0.35078934,   0.36264776],\n",
       "        [ -0.05830167,   0.19382316,   0.52204047],\n",
       "        [ -0.53904356,  -0.48237264,   0.05643711],\n",
       "        [ -0.3450306 ,   0.20168615,   0.14367766],\n",
       "        [  0.22663123,  -0.07063363,  -0.08430016]]),\n",
       " array([-9.00266907e-05, -2.54407778e-04, -9.13840214e-05]),\n",
       " array([[-0.79013991],\n",
       "        [-0.13710218],\n",
       "        [-1.45952371]]),\n",
       " array([-0.17650805]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = model.ws[-1]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 datos de entrada de prueba (X_test):\n",
      "[[   0    3  133   11    0    3   10   42    0    0    0    2    0]\n",
      " [   0   10  692   26    0    2    8  125    0    0    0    3    0]\n",
      " [   0    6  518   22    0    3   16  117    0    0    0    4    0]\n",
      " [   0   12  777   27    0    2    7  134    0    0    0    3    0]\n",
      " [   1   17 1064   32    0    2    6  112    0    0    0    4    0]]\n",
      "Primeras 5 etiquetas de prueba (y_test):\n",
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeros 5 datos de entrada de prueba (X_test):\")\n",
    "print(x_test[:5])\n",
    "\n",
    "print(\"Primeras 5 etiquetas de prueba (y_test):\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.58974398e-100])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nuevo punto\n",
    "x_new = x_test\n",
    "X_new = [6.53533935546875,619.587844574841,24.891521539971,0.109059009370733,4.27647702615029,18.9005747905553,98.6139705882353,0.293314496659804,0.086033393950794,0.530941131652007,4.47334558823529,0.981938696883038,7.45834073119875E-155]\n",
    "\n",
    "y_pred = model.predict(w, X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(perceptron, x, t = 0.5):\n",
    "    w = perceptron.ws[-1]\n",
    "    x = np.c_[np.ones(len(x)), x]\n",
    "    y = perceptron(w, x)\n",
    "    return (y > t).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    return np.sum(y_pred == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perceptron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[39m=\u001b[39m evaluate(perceptron, x_train)\n\u001b[0;32m      2\u001b[0m accuracy(y_pred, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'perceptron' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = evaluate(perceptron, x_train)\n",
    "accuracy(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perceptron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[39m=\u001b[39m evaluate(perceptron, x_test)\n\u001b[0;32m      2\u001b[0m accuracy(y_pred,y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'perceptron' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = evaluate(perceptron, x_test)\n",
    "accuracy(y_pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
